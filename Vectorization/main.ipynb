{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.vertices = []\n",
    "        self.edges = []\n",
    "        self.adjacency_list = {}\n",
    "    \n",
    "    def readGraph(self,fileName:str):\n",
    "        try:\n",
    "            with open(fileName, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        nodes = line.split(':')\n",
    "                        if len(nodes) == 2:\n",
    "                            source = nodes[0].strip()\n",
    "\n",
    "                            if(nodes[1]==\"\"):\n",
    "                                continue    \n",
    "\n",
    "                            self.vertices.append(source)\n",
    "                            destinations = nodes[1].split(',')\n",
    "                            destinations = [dest.strip() for dest in destinations]\n",
    "                            self.adjacency_list[source] = destinations\n",
    "                            \n",
    "                            for x in destinations:\n",
    "                                if (source,x) or (x,source) not in self.edge.items():\n",
    "                                    self.edges.append((x,source))\n",
    "                                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {fileName}\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Error: Unable to decode file at {fileName}\")\n",
    "    \n",
    "\n",
    "    def find_Similarity(self,edges:list):\n",
    "       # Convert each tuple to a set\n",
    "        x_sets = [set(pair) for pair in self.edges]\n",
    "        y_sets = [set(pair) for pair in edges]\n",
    "\n",
    "        # Find sets in x that are also in y\n",
    "        similar_sets = [pair for pair in x_sets if pair in y_sets]\n",
    "\n",
    "        # print(\"Sets with similar elements:\")\n",
    "        # for pair in similar_sets:\n",
    "        #     print(pair)\n",
    "        return similar_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/BusinessandFinance/anchorwm.txt\n",
      "../data/BusinessandFinance/honda.txt\n",
      "../data/HealthandFitness/australianeggs.txt\n",
      "../data/HealthandFitness/careHospitals.txt\n",
      "../data/HealthandFitness/decathlon.txt\n",
      "../data/HealthandFitness/finemancook.txt\n",
      "../data/HealthandFitness/healthline.txt\n",
      "../data/HealthandFitness/healthShots.txt\n",
      "../data/HealthandFitness/lybrate.txt\n",
      "../data/HealthandFitness/marham.txt\n",
      "../data/HealthandFitness/medicalnewstoday.txt\n",
      "../data/HealthandFitness/myProtien.txt\n",
      "../data/HealthandFitness/ndtv.txt\n",
      "../data/HealthandFitness/pharmeasy.txt\n",
      "../data/ScienceandEducation/bcc-campus.txt\n",
      "../data/ScienceandEducation/britannica.txt\n",
      "../data/ScienceandEducation/maqurieUniversity.txt\n",
      "../data/ScienceandEducation/merriam-webster.txt\n",
      "../data/ScienceandEducation/twi-global.txt\n",
      "../data/ScienceandEducation/UniversityofBath.txt\n",
      "../data/ScienceandEducation/wikipedia.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"../data\"\n",
    "items = os.listdir(path)\n",
    "\n",
    "\n",
    "directories = [item for item in items if os.path.isdir(os.path.join(path, item))]\n",
    "\n",
    "def get_files(path):\n",
    "    files = []\n",
    "    for root, directories, filenames in os.walk(path):\n",
    "        files.extend(filenames)\n",
    "    return files\n",
    "\n",
    "def readFile(file):\n",
    "    print(file)\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.readlines()\n",
    "    return \"\".join(text)\n",
    "\n",
    "train=[]\n",
    "for directory in directories: # types of graph\n",
    "    for file in get_files(path+\"/\"+directory):\n",
    "        data=readFile(path+\"/\"+directory+\"/\"+file)\n",
    "        train.append({\"data\":data,\"title\":directory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "def separate_set(data):\n",
    "    return [x['data'] for x in data], [x['title'] for x in data]\n",
    "\n",
    "def make_vectors(train):\n",
    "    vector = TfidfVectorizer()\n",
    "    train_vectors = vector.fit_transform(train)\n",
    "    return train_vectors\n",
    "\n",
    "# Assuming 'train' contains your data\n",
    "X_train, X_test = train_test_split(train)\n",
    "\n",
    "# Separate text and title for training and testing sets\n",
    "text_train, title_train = separate_set(X_train)\n",
    "text_test, title_test = separate_set(X_test)\n",
    "\n",
    "# Vectorize the text data\n",
    "train_vectors = make_vectors(text_train)\n",
    "test_vectors = make_vectors(text_test)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and SVM classifier\n",
    "model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(text_train, title_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predicted_titles = model.predict(text_test)\n",
    "\n",
    "# Calculate accuracy (if you have ground truth labels for the test data)\n",
    "accuracy = accuracy_score(title_test, predicted_titles)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
